{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Pytorch keypoints RCNN**"
      ],
      "metadata": {
        "id": "6GlwRq8YyNr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**17 keypoints:**[ ‘nose’, ‘left_eye’, ‘right_eye’, ‘left_ear’, ‘right_ear’, ‘left_shoulder’, ‘right_shoulder’, ‘left_elbow’, ‘right_elbow’, ‘left_wrist’, ‘right_wrist’, ‘left_hip’, ‘right_hip’, ‘left_knee’, ‘right_knee’, ‘left_ankle’, ‘right_ankle’]\n",
        "\n",
        "**edges =** [\n",
        "    (0, 1), (0, 2), (2, 4), (1, 3), (6, 8), (8, 10),\n",
        "    (5, 7), (7, 9), (5, 11), (11, 13), (13, 15), (6, 12),\n",
        "    (12, 14), (14, 16), (5, 6)\n",
        "]\n",
        "\n",
        "PyTorch pre-trained models for human pose and keypoint detection\n"
      ],
      "metadata": {
        "id": "WOZ7qH-FyoKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libs**"
      ],
      "metadata": {
        "id": "3R0lva9AzGWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import cv2\n",
        "import argparse\n",
        "from PIL import Image\n",
        "from torchvision.transforms import transforms as transforms\n",
        "import torchvision\n",
        "import os\n",
        "import matplotlib\n"
      ],
      "metadata": {
        "id": "mjCCCVirzFna"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(min_size=800):\n",
        "    # initialize the model\n",
        "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True,\n",
        "                                                                   num_keypoints=17,\n",
        "                                                                   min_size=min_size)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "IFIntJqN0Bxn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model onto the computation device and set it to eval mode\n",
        "# Set the computation device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = get_model().to(device).eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTcycoK23lca",
        "outputId": "daa5b7a7-8ce4-451e-860e-f85d31de37ee"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=KeypointRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=KeypointRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWrQDs8B3xYD",
        "outputId": "9625d9dc-765a-4002-f941-1fba5dc4e93f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive_image_dir =\"/content/drive/MyDrive/imgs_test_set\"\n",
        "output_dir = \"/content/drive/MyDrive/imgs_test_set/2D_pose/pytorch_rcnn\""
      ],
      "metadata": {
        "id": "k_g9tXH9zKIU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transform to convert the image to a tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "i95wLX3z1AL7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edges = [\n",
        "    (0, 1), (0, 2), (2, 4), (1, 3), (6, 8), (8, 10),\n",
        "    (5, 7), (7, 9), (5, 11), (11, 13), (13, 15), (6, 12),\n",
        "    (12, 14), (14, 16), (5, 6)\n",
        "]"
      ],
      "metadata": {
        "id": "y1iRTujjNUcJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_keypoints_and_boxes(outputs, image):\n",
        "    for output in outputs:\n",
        "        # Get the detected keypoints, bounding boxes, and confidence scores\n",
        "        keypoints = output['keypoints']\n",
        "        boxes = output['boxes']\n",
        "        scores = output['scores']\n",
        "\n",
        "        # Iterate through instances\n",
        "        for i in range(len(keypoints)):\n",
        "            if scores[i] > 0.9:  # Proceed if confidence is above 0.9\n",
        "                keypoints_array = keypoints[i].cpu().detach().numpy()\n",
        "                keypoints_array = keypoints_array[:, :].reshape(-1, 3)\n",
        "                for p in range(keypoints_array.shape[0]):\n",
        "                    # Draw the keypoints\n",
        "                    cv2.circle(image, (int(keypoints_array[p, 0]), int(keypoints_array[p, 1])),\n",
        "                                3, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)\n",
        "\n",
        "                # Draw the lines joining the keypoints\n",
        "                for ie, e in enumerate(edges):\n",
        "                    # Get different colors for the edges\n",
        "                    rgb = matplotlib.colors.hsv_to_rgb([\n",
        "                        ie / float(len(edges)), 1.0, 1.0\n",
        "                    ])\n",
        "                    rgb = rgb * 255\n",
        "                    # Join the keypoint pairs to draw the skeletal structure\n",
        "                    cv2.line(image, (int(keypoints_array[e[0], 0]), int(keypoints_array[e[0], 1])),\n",
        "                             (int(keypoints_array[e[1], 0]), int(keypoints_array[e[1], 1])),\n",
        "                             tuple(rgb), 2, lineType=cv2.LINE_AA)\n",
        "\n",
        "                # Draw the bounding boxes around the objects\n",
        "                box = boxes[i].cpu().detach().numpy()\n",
        "                cv2.rectangle(image, (int(box[0]), int(box[1]), int(box[2]), int(box[3])),\n",
        "                              color=(0, 255, 0),\n",
        "                              thickness=2)\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "dCVsPqiONCHx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through all images in the input directory\n",
        "for filename in os.listdir(drive_image_dir):\n",
        "    if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        image_path = os.path.join(drive_image_dir, filename)\n",
        "\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        # NumPy copy of the image for OpenCV functions\n",
        "        orig_numpy = np.array(image, dtype=np.float32)\n",
        "        # Convert the NumPy image to OpenCV BGR format\n",
        "        orig_numpy = cv2.cvtColor(orig_numpy, cv2.COLOR_RGB2BGR) / 255.\n",
        "        # Transform the image\n",
        "        image = transform(image)\n",
        "        # Add a batch dimension\n",
        "        image = image.unsqueeze(0).to(device)\n",
        "\n",
        "        # Get the detections, forward pass the image through the model\n",
        "        with torch.no_grad():\n",
        "            outputs = model(image)\n",
        "\n",
        "        # Draw the keypoints, lines, and bounding boxes\n",
        "        output_image = draw_keypoints_and_boxes(outputs, orig_numpy)\n",
        "\n",
        "        # Set the save path for the output image\n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        cv2.imwrite(save_path, output_image * 255.)"
      ],
      "metadata": {
        "id": "mBh6ckoq4Aeu"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}